#statistica 

#### Valore atteso calcolato con integrale?

#### Disuguaglianza di Markov

Se $X$ è una variabile aleatoria che non è mai negativa, allora per ogni $a \gt 0$,
$$
\mathbb{P}(X \ge a) \le \frac{\mathbb{E}(X)}{a}
$$
*Dimostrazione:*
$$
\begin{aligned}
\mathbb{E}(X) &= \sum_{x \in D_X}xf_X(x) \\
&= \sum_{x < a \in D_X}xf_X(x) + \sum_{x \ge a \in D_X}xf_X(x) \\
&\ge \sum_{x \ge a \in D_X}xf_X(x) \\
&\ge \sum_{x \ge a \in D_X}af_X(x) = a\sum_{x \ge a \in D_X}f_X(x) = a\mathbb{P}(X \ge a) \\
&\rightarrow \mathbb{E}(X) \ge a\mathbb{P}(X \ge a)
\end{aligned}
$$

$$
\begin{aligned}
\mathbb{P}(X \lt a) &= 1 - \mathbb{P}(X \ge a) \\
&\ge 1 - \frac{\mathbb{E}(X)}{a}
\end{aligned}
$$
Inverto il segno della disequazione perché la probabilità di $X \ge a$ è negativa

#### Disuguaglianza di Tchebyshev

Se $X$ è una variabile aleatoria che non è mai negativa con media $\mu$ e varianza $\sigma^2$, allora per ogni $r$ vale
$$
\mathbb{P}(|X - \mu| \ge r) \le \frac{\sigma^2}{r^2}
$$

*Dimostrazione:*
	Dato che ho r > 0 posso scrivere
$$
\begin{aligned}
&|X - \mu| \ge r \leftrightarrow (X-\mu)^2 \ge r^2 \\
&=\mathbb{P}((X-\mu)^2 \ge r^2) \\
&(X-\mu)^2 = Y \\
&= \mathbb{P}(Y \ge r^2) \le \frac{\mathbb{E}(Y)}{r^2} = \frac{\sigma^2}{r^2}
\end{aligned}
$$

Se nella disuguaglianza si pone $r = k\sigma$, essa assume la forma seguente:
$$
\mathbb{P}(|X - \mu| \ge k\sigma) \le \frac{1}{k^2}
$$
in altri termini, la probabilità che una variabile aleatoria differisca dalla sua media per più di k volte la deviazione standard, non può superare il valore $1/k^2$ .

L'importanza delle disuguaglianze di Markov e Tchebyshev, sta nel fatto che permettono di limitare le probabilità di eventi rari che riguardano variabili aleatorie di cui conosciamo solo la media, oppure la media e la varianza.
 
#### Variabili aleatorie di Bernoulli

Una variabile aleatoria $X$ si dice *di Bernoulli* se la sua funzione di massa di probabilità è del tipo
$$
\begin{aligned}
&\mathbb{P}(X = 0) = 1 - p \\
&\mathbb{P}(X = 1) = p
\end{aligned}
$$
dove $0 \le p \le 1$ indica la probabilità del "successo" dell'esperimento.

Una variabile aleatoria bernoulliana può assumere solo i valori $0$ e $1$. Il suo valore atteso di conseguenza è dato da
$$
\mathbb{E}(X) = \sum_{x \in D_X} xf_X(x) = 0 \cdot f_X(0) + 1 \cdot f_X(1) = p 
$$
ed è quindi pari alla probabilità di ottenere un successo ($X = 1$).

La varianza di una variabile aleatoria bernoulliana è data da
$$
\text{Var}(X) = \mathbb{E}(X^2)-\mathbb{E}(X)^2 = \mathbb{E}(X)-\mathbb{E}(X)^2 = p - p^2 = p(1 - p)
$$
perché $\mathbb{E}(X^2) = \mathbb{E}(X) = p$.

Chiameremo *variabile aleatoria **binomiale*** di parametri $(n,p)$ una variabile aleatoria $X$ che denota il numero totale di successi di $n$ ripetizioni indipendenti di un esperimento, che ovviamente può concludersi in un "successo" con probabilità $p$ o in un "fallimento" con probabilità $1 - p$. 

Indicheremo la funzione di massa di probabilità di una variabile aleatoria *binomiale* $X \sim B(n,p)$:
$$
f_X(i) = \mathbb{P}(X = i) = p^i(1-p)^{n-i}\binom{n}{i}\cdot I_{0,...,n}(i)
$$
Si noti che la somma di tutti i valori possibili che può assumere una variabile aleatoria binomiale è pari a $1$ 
$$
\sum_{i}f_X(i) = \sum_{i = 0}^{n}p^i(1-p)^{n-i} = (p + 1 - p)^n = 1^n
$$
>
>N.B. $\sum_{i = 0}^{n}p^i(1-p)^{n-i}$ è il binomio di Newton
>

