#statistica 

#### Statistica inferenziale

Per statistica inferenziale (o inferenza statistica) si intende l’insieme di tecniche statistiche che permettono di  generalizzare i risultati ottenuti dai dati raccolti su un *campione* alla *popolazione* da cui è stato estratto.

Un insieme di variabili aleatorie indipendenti, tutte con la stessa distribuzione $F$, si dice *campione* della distribuzione $F$.

In statistica la distribuzione $F$ non è mai completamente nota (a differenza in probabilità si conoscono le distribuzioni in gioco), è possibile usare i dati per fare *inferenza* su $F$. E' possibile che in alcuni casi che la distribuzione sia nota ma non siamo a conoscenza di alcuni parametri, questo tipo di problemi vengono chiamati di inferenza ***parametrica***, mentre a volte capita di trovare dei problemi in cui non conosciamo nulla della distribuzione $F$, in questo caso daremo il nome di inferenza ***non parametrica***.

Dunque per stimare un qualcosa di sconosciuto in un campione dovremo definire una funzione che stimi $\tau(\Theta)$ con qualcosa che dipende da $\Theta$.
Lo stimatore è così definito
$$
\begin{aligned}
&t: D_X^n \rightarrow \mathbb{R} \\
&T = t(X_1,X_2,...,X_n) = \hat\tau \text{ stima} \\
&\hat\tau \approx t(\Theta)
\end{aligned}
$$
*esempio:* $X \sim B(p)$   $\Theta = p$   e voglio conoscere $p$
Prendo il campione $X_1,...,X_n$   $\tau(\Theta) = \tau(p) = p$   $p$ dipende da se stesso
la statistica / stimatore $t(X_1,...,X_n) = \frac{1}{n}\sum_{i=1}^n X_i = \overline X$.
$\overline x \approx p$ 

$\overline X$ è la media campionaria, essa è una funzione delle variabili aleatorie, in quanto tale è anche essa una variabile aleatoria. Ha senso quindi domandarsi quanto vale il suo valore atteso.

$\mathbb{E}[\overline X]$ è un indice di centralità ...

Se ciò che voglio stimare è $\Theta$ con $\tau(\Theta) = \Theta$ e $t(X_1,...,X_n) = T$. Quando accade che $\mathbb{E}(T) = \tau(\Theta)$ diciamo che $t$ è uno stimatore *non deviato* (o ***unbiased*** o *non distorto*) di $\tau(\Theta)$.

E' possibile anche calcolare la varianza di $\overline X$ 
$$
\begin{aligned}
&\text{Var}(\overline X) = \text{Var}(\frac{1}{n}\sum_{i=1}^n X_i) = \frac{1}{n^2}\sum_{i=1}^n\text{Var}(X_i) = \frac{1}{n^2}\sum_{i}\text{Var}(X) = \frac{\sigma^2}{n}
\end{aligned}
$$
Le oscillazioni che la stima fa attorno al valore centrale crescono all'aumentare della popolazione e diminuiscono se scende.

#### MSE: Mean Squared Error

Per capire se la statistica che stiamo usando è corretta possiamo utilizzare l'MSE
$$
\text{MSE} = \mathbb{E}[(T-\tau(\Theta))^2] \text{ con } T = t(X_1,...,X_n)
$$

$$
\begin{aligned}
\mathbb{E}[(T-\tau(\Theta))^2] &= \mathbb{E}[(T-E[T]+E[T]-\tau(\Theta))^2] \\
& = \mathbb{E}[(T-E[T])^2+ 2(T-E[T])(E[t]-\tau(\Theta)) + (E[T]-\tau(\Theta))^2] \\
& = \mathbb{E}[(T-E[T])^2]+ \cancel{2(T-E[T])(E[t]-\tau(\Theta))} + (E[T]-\tau(\Theta))^2 \\
& = \text{Var}(T) + (E[T]-\tau(\Theta))^2
\end{aligned}
$$

Chiamiamo *bias* lo scarto (non quadratico) $b_{\tau(\Theta)} (T) = \mathbb{E}(T)-\tau(\Theta)$, possiamo quindi riscrivere $\text{MSE}_{\tau(\Theta)}(T) = \text{Var}(T)+b(T)_{\tau(\Theta)} ^2$

Se $T$ è uno stimatore non deviato per $\tau(\Theta) \rightarrow b_{\tau(\Theta)} (T) = 0$. Di conseguenza $\text{MSE}_{\tau(\Theta)}(T) = \text{Var}(T)$, questo ci permette di capire quanto lo stimatore oscilla attorno al valore che vogliamo stimare.

#### Consistenza in media quadratica

Uno stimatore $T$ è consistente in media quadratica per $\lim_{n\rightarrow \infty}\text{MSE}_{\tau(\Theta)}(T) = 0$. Se cambia n cambia il numero di argomenti di $\tau$, quindi per ogni $n$ ha una diversa $\tau$. 