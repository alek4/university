#statistica 

#### Funzione di massa di probabilità

Data una variabile casuale discreta $X$, la funzione di probabilità è la funzione 
$$
\begin{aligned}
&f_{X} : \mathbb{R} \rightarrow [0,1] \\
&f_{X}(x)=P(X=x)
\end{aligned}
$$

*Proprietà*:
- $\forall x \in \mathbb{R}$ $f_{X}(x) \ge 0$
- $\sum_{x \in D}f_{X}(x_i) =1$ 

> ci sono da elencare delle proprietà in relazione alla funzione di ripartizione

#### Valore atteso

Sia $X$ una variabile aleatoria con supporto $D = \{x_1, x_2, ...\}$, il valore atteso di $X$ è il numero
$$
\mathbb{E}(X) = \sum_{x \in D}x_i\mathbb{P}(X=x_i)
$$
In altri termini, si tratta della media pesata dei valori possibili di $X$.

*Proposizione 1*:
Se $X$ è una variabile aleatoria discreta, per ogni funzione reale $g$,
$$
\mathbb{E}[g(X)] = \sum_{x \in D}g(x_i)\mathbb{P}(X=x_i)
$$
*Corollario 1*:
Per ogni coppia di costanti reali $a$ e $b$,
$$
\begin{aligned}
\mathbb{E}[aX + b] &= \sum_{x}(ax_i+b)\mathbb{P}(X=x_i) \\
&= \sum_{x}ax_i\mathbb{P}(X=x_i) + \sum_{x}b\mathbb{P}(X=x_i) \\
&= a\sum_{x}x_i\mathbb{P}(X=x_i) + b\sum_{x}\mathbb{P}(X=x_i) \\
&= a\mathbb{E}(X)+b
\end{aligned}
$$
>*casi particolari*:
>	$a = 0 \rightarrow \mathbb{E}(b) = b$
>	$b = 0 \rightarrow \mathbb{E}(aX) = a\mathbb{E}(X)$

#### Varianza

Sia $X$ una variabile aleatoria con media $\mu$. La varianza di $X$, che si denota con $\text{Var}(X)$, se esiste è la quantità
$$
\text{Var}(X) := \mathbb{E}[(X-\mu)^2]
$$
Scrivendola in modo più comodo
$$
\begin{aligned}
\text{Var}(X) :&= \mathbb{E}[(X-\mu)^2]\\
&= \mathbb{E}[X^2-2\mu X+\mu^2] \\
&= \mathbb{E}[X^2] - 2\mu\mathbb{E}[X]+\mu^2 \\
&= \mathbb{E}[X^2] - \mu^2
\end{aligned}
$$
Ovvero,
$$
\text{Var}(X) = \mathbb{E}[X^2] - \mathbb{E}[X]^2
$$

*Corollario 2*:
Per ogni coppia di costanti reali $a$ e $b$, ricordando che $\mathbb{E}[aX+b] = a\mathbb{E}[X]+b = a\mu+b$
$$
\begin{aligned}
\text{Var}(aX + b) :&= \mathbb{E}[(aX+b-\mathbb{E}[aX+b])^2] \\
&= \mathbb{E}[(aX+b-a\mu-b)^2] \\
&= \mathbb{E}[a^2(X-\mu)^2] \\
&= a^2\mathbb{E}[(X-\mu)^2] \\
&= a^2\text{Var}(X)
\end{aligned}
$$
> *caso particolari*:
> 	$a = 0, \text{Var}(b) = 0$

*Definizione*: chiamiamo la quantità $\sqrt{\text{Var}(X)}$ ***deviazione standard*** della variabile aleatoria $X$.

#### Variabili aleatorie multivariate

Siano $X$ e $Y$ due variabili aleatorie che riguardano lo stesso esperimento casuale. Si dice ***funzione di ripartizione congiunta*** di $X$ e $Y$ la funzione seguente
$$
F(X,Y) = \mathbb{P}(X \le x, Y \le y)
$$
dove la virgola nell'argomento di $\mathbb{P}()$ denota l'intersezione tra eventi (and).

Questa funzione ci permette di calcolare le probabilità di tutti gli eventi che dipendono, singolarmente o congiuntamente, da $X$ e $Y$.

*esempio:*
$$
\begin{aligned}
F_X(x) &= \mathbb{P}(X \le x)\\
&= \mathbb{P}(X \le x, Y \le \infty)\\
&= F(x, \infty)
\end{aligned}
$$
(*questo vale anche per Y*)

Se $X$ e $Y$ sono variabili aleatorie discrete che assumono i valori $x_1,x_2...$ e $y_1,y_2,...$ rispettivamente, la funzione 
$$
f(x_i,y_j) = \mathbb{P}(X=x_i, Y=y_i)
$$
è la loro funzione di ***massa di probabilità congiunta***.

Dalla funzione di massa di probabilità congiunta è possibile ricavare le funzioni di massa individuali di $X$ e $Y$. Dunque possiamo vedere l'evento $\{X = x_i\}$ come l'unione al variare di $j$ degli eventi $\{X = x_i, Y = y_j\}$, che sono *mutualmente esclusivi*
$$
\{X = x_i\} = \bigcup_j\{X = x_i, Y = y_i\}
$$
grazie all'Assioma 3,
$$
\begin{aligned}
f_X(x_i) &= \mathbb{P}(X = x_i) \\
&=\mathbb{P}(\bigcup_j\{X = x_i, Y = y_i\}) \\
&=\sum_j\mathbb{P}(X = x_i, Y=y_j)\\
&=\sum_j f(x_i, y_j)
\end{aligned}
$$
Analogamente tutto questo vale anche per $f_Y$, anche se abbiamo dimostrato che dalla funzione di massa di probabilità congiunta possiamo ricavare le funzioni di massa di probabilità individuali (anche dette *marginali*), il contrario non è vero.

#### Indipendenza di variabili aleatorie

Due variabili aleatorie che riguardano lo stesso esperimento casuale si dicono *indipendenti* se, per ogni coppia di insiemi di numeri reali $A$ e $B$, è soddisfatta l'equazione
$$
\mathbb{P}(X \in A, Y \in B) = \mathbb{P}(X \in A)\mathbb{P}(Y \in B)
$$
ovvero, se per ogni scelta di $A$ e $B$, gli eventi $\{X \in A\}$ e $\{Y \in B\}$ risultano indipendenti. In caso contrario $X$ e $Y$ si dicono *dipendenti*.

Usando gli assiomi della probabilità possiamo dimostrare che la funzione di ripartizione congiunta sia il prodotto delle marginali
$$
F(a,b) = F_X(a)F_Y(b)
$$
Se le variabili aleatorie sono discrete, l'indipendenza vale anche per la funzione di massa di probabilità congiunta
$$
f(x,y) = f_X(x)f_Y(y)
$$
