#intelligenza-artificiale 

Reinforcement learning involves an agent interacting with its environment and receiving rewards based on its performance. The goal is to maximize the expected sum of these rewards. This is similar to the concept of rewards in [[Markov decision processes (MDP)|Markov decision processes]] (MDPs). However, in reinforcement learning, the agent is not given the MDP as a problem to solve; rather, it is actively participating in the MDP and must learn about the transition model and reward function through its actions.

When choosing an action in reinforcement learning, there are two main approaches: **_exploration_** and **_exploitation_**. Exploration involves prioritizing the acquisition of new information about the model, even if it may lead to mistakes. Exploitation involves prioritizing performance based on what has already been learned about the model, in an attempt to avoid mistakes. It is important to note that the learned model is not always exactly the same as the real model, but rather is an estimate. Choosing the optimal action based on the learned model may not be as good as choosing the optimal action based on the real model, and the difference in utility between the two is called regret.

### Passive RL

In this scenario, the agent is a passive learning agent because it is not actively taking actions in the environment, but rather is learning about the utility function based on its fixed policy. The environment is fully observable, meaning that the agent has complete information about the state of the environment at all times, and there are a small number of actions and states. The agent's fixed policy, called $\pi(s)$, determines its actions, and the agent is trying to learn the value function, $V^\pi(s)$, which is the expected total discounted reward if this policy is followed starting in state s.

To determine the optimal policy and evaluate the performance of a given policy, we will use Adaptive Dynamic Programming (ADP). The agent will generate a dataset of episodes by exploring the model, where each episode is a sequence of transitions and rewards of fixed maximum length obtained by following the given policy. Each transition has the form: $s_1$ --$R(s_1,\pi(s_1),s_2)$-->$s_2$. 
From this generated dataset, we can estimate the model by calculating the number of times the agent was in state $s$ and took action $a$: #$(s,a)$ and the number of times it was in state $s$, took action $a$, and ended up in state $s'$: #$(s,a,s')$. Therefore, we can estimate the transition model: $\hat{P}(s' | s,a) = \frac{\#(s,a,s')}{\#(s,a)}$ and some rewards: $\hat{R}(s,a,s') = R(s,a,s')$. Now, we can perform policy evaluation of $\pi$ within the generated MDP $\langle\hat{P},\hat{R}\rangle$ using methods such as solving a system of linear equations or [[Programmazione dinamica|dynamic programming]].

$$
V^\pi(s) = Q(s, \pi(s)) = \sum_{s'}\hat{P}(s'|s,\pi(s))[\hat{R}(s,\pi(s),s')+\gamma V^\pi(s')]
$$
Model-Based reinforcement learning involves learning the model in order to use techniques that assume knowledge of the model. The method improves as the number of episodes in the training set increases, as the empirical frequencies derived from observations converge towards the true transition probabilities. A better estimate of the model leads to a more realistic evaluation of the policy. However, when the underlying MDP has a very large number of states, it may require a very high number of episodes to obtain a reasonable estimate of the model, and exploration may take too long. The method also requires maintaining a count of visits and transitions, which may require a large amount of memory. An alternative approach, Model-Free reinforcement learning, does not estimate the model but instead directly estimates the state-value function.